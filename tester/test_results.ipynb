{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# consts\n",
    "# number of iterations:\n",
    "config = open(\"tester_config\", \"r\")\n",
    "times = int(config.readline()[6:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test_result.txt\", sep=' ', header=None, names=[\"lib\", 'test', 'test_name'] + [i for i in range(times)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = [i for i in range(times)]\n",
    "data[\"mean\"] = data[values].mean(axis=1)\n",
    "data[\"std\"]  = data[values].std(axis=1)\n",
    "data[\"testid\"] = data[\"test\"] + \"_\" + data[\"test_name\"]\n",
    "data = data.reindex_axis(sorted(data.columns), axis=1)\n",
    "tests = sorted(list(set(data['testid'].tolist())))\n",
    "libs = sorted(list(set(data['lib'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tester results.\n",
    "### Current system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Version:\", platform.platform()\n",
    "print \"CPU cores:\", multiprocessing.cpu_count()\n",
    "print \"Page size:\", os.sysconf('SC_PAGE_SIZE')\n",
    "print \"Current time:\", datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rankings\n",
    "** Test passage points**:\n",
    "* **10 points** for being in std of the best test performer **or**\n",
    "* **5 - 10 points** for passing the test according to time,\n",
    "* **-5** for failed test,\n",
    "* **-2** for timeout.\n",
    "\n",
    "**BONUS**: \n",
    "* **2** for best std in each test,\n",
    "* **-3** for _std = 100 * best std_ in this test,\n",
    "* **5** for all tests passage.\n",
    "\n",
    "## Legend\n",
    "* **Green** - passed successfully,\n",
    "* **Yellow** - timeout,\n",
    "* **Red** - failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_group_best = 10\n",
    "p_group_worst = 5\n",
    "\n",
    "p_timeout = 0\n",
    "p_fail = -5\n",
    "\n",
    "p_best_std = 1\n",
    "p_bad_std_mul = 100\n",
    "p_bad_std = -2\n",
    "p_all_passed = 5\n",
    "p_all_passed_with_timeout = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks = pd.DataFrame(index=libs, columns=tests).fillna(p_fail)\n",
    "ranks['All test passed'] = np.zeros(len(libs))\n",
    "\n",
    "datarank = data[['lib', 'testid', 'mean', 'std']]\n",
    "for test in tests:\n",
    "    testf = datarank[datarank.testid == test]\n",
    "    best_mean = testf['mean'][testf['mean'] != -1].min()\n",
    "    worst_mean = testf['mean'][testf['mean'] != -1].max()\n",
    "    best_mean_std = testf[testf['mean'] == best_mean][\"std\"].values\n",
    "    worst_mean_std = testf[testf['mean'] == worst_mean][\"std\"].values\n",
    "    \n",
    "    if len(best_mean_std) == 0:\n",
    "        # All libs failed test:\n",
    "        continue\n",
    "    else:\n",
    "        best_mean_std = best_mean_std[0]\n",
    "        worst_mean_std = worst_mean_std[0]\n",
    "        \n",
    "    angle = (p_group_best - p_group_worst) / (best_mean - worst_mean)\n",
    "        \n",
    "    best_std = testf['std'][testf['std'] != 0].min()\n",
    "    \n",
    "    for index, row in testf.iterrows():\n",
    "        if row[\"mean\"] != -1:     \n",
    "            if (np.abs(row[\"mean\"] - best_mean) <= np.abs(row['std'] + best_mean_std)):\n",
    "                ranks.set_value(row['lib'], row['testid'], p_group_best)\n",
    "            else: \n",
    "                ranks.set_value(row['lib'], row['testid'], p_group_best - int((best_mean - row['mean']) * angle))\n",
    "        else:\n",
    "            ranks.set_value(row['lib'], row['testid'], p_timeout)\n",
    "            \n",
    "        # best std:\n",
    "        if row[\"std\"] == best_std:\n",
    "            ranks.loc[row['lib'], row['testid']] += p_best_std\n",
    "            \n",
    "        # bad std:\n",
    "        if row[\"std\"] > p_bad_std_mul * best_std:\n",
    "            ranks.loc[row['lib'], row['testid']] += p_bad_std\n",
    "            \n",
    "all_passed = ranks[ranks <= p_timeout].any(axis=1)\n",
    "\n",
    "for i, item in all_passed.iteritems():\n",
    "    if item == False: \n",
    "        ranks.set_value(i, 'All test passed', p_all_passed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = [\"pale red\", \"pale red\", \"pale red\", \"pale red\", \"pale red\", \"amber\", \"amber\", \"amber\", \"amber\"]\n",
    "cmap = matplotlib.colors.ListedColormap(sns.xkcd_palette(colors) + sns.light_palette(\"green\", n_colors=8)[1:])\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.plt.title(\"Points for tests\", fontsize=80, y=1.03)\n",
    "sns.heatmap(ranks, annot=True, cmap=cmap, center=0, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Code to shift \"Test passage\" plot to left\n",
    "# from IPython.display import HTML\n",
    "# HTML(\"\"\"<script>\n",
    "#   $(document).ready(function(){\n",
    "#     $('div.prompt').hide();\n",
    "#     $('nav#menubar').hide();\n",
    "#   });\n",
    "# </script>\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks[\"sum\"] = ranks.sum(axis=1)\n",
    "max_score = ranks[\"sum\"].max()\n",
    "ranks[\"sum\"] = ranks[\"sum\"] * 30 / max_score\n",
    "\n",
    "\n",
    "ranks.sort_values(by=['sum'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(font_scale=2)\n",
    "sns.plt.title(\"Final ranks\", fontsize=50, y=1.03)\n",
    "sns.barplot(y=ranks[\"sum\"], x=ranks.index)\n",
    "plt.ylabel('Points')\n",
    "plt.yticks([i*2 for i in range(16)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results by each test.\n",
    "There are only libs that passed the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data without timeout values.\n",
    "passed_data = data[data[\"mean\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val = passed_data.set_index(passed_data[\"testid\"]).drop([\"test\", \"test_name\", \"mean\", \"std\", 'testid'], axis=1)\n",
    "sns.set(font_scale=2)\n",
    "for test in tests:\n",
    "    a = val[val.index == test]\n",
    "    a = a.set_index(a['lib']).drop([\"lib\"], axis=1).transpose()\n",
    "    if len(a.columns) == 0:\n",
    "        print \"\\nAll libs have failed test %s.\\n\" % (test)\n",
    "        continue\n",
    "    plt.figure(figsize=(25, 8))\n",
    "    sns.plt.title(test, fontsize=30, y=1.03)\n",
    "    ax = sns.boxplot(data=a)\n",
    "    sns.swarmplot(data=a, size=5, ax=ax, color=\"black\")\n",
    "    plt.ylabel('Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw results \n",
    "with mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', len(data))\n",
    "data.drop(values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter by dokhlopkov.\n",
    "</footer>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
